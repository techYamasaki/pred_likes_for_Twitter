{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pred_likes_with_multi_keyword_affine.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "m7TTiCYmIoTW",
        "GkEL5dDWXr0q",
        "YmKdM0NgqSl4",
        "9UUTjMt6Bwpq"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7TTiCYmIoTW"
      },
      "source": [
        "# データ読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5lfYJ8oIrdR"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkEL5dDWXr0q"
      },
      "source": [
        "# 前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmKdM0NgqSl4"
      },
      "source": [
        "## モデル定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpnAQ8MnuLar"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "s_scaler = StandardScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sccyldpUEbY"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6Uojxt7pl-6"
      },
      "source": [
        "# MLP_for_Basic\n",
        "basic_size = 7\n",
        "\n",
        "Basic_MLP_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, input_shape=(basic_size,)),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.Dense(32),\n",
        "    tf.keras.layers.Activation('relu')\n",
        "])\n",
        "\n",
        "Basic_MLP_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFw5Y7fOMmd1"
      },
      "source": [
        "# MLP_for_multi_keyword\n",
        "vec_size = 200\n",
        "#max_length = 10\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(None, vec_size))\n",
        "x1 = tf.keras.layers.Masking(mask_value=0.0)(inputs)\n",
        "x1 = tf.keras.layers.Dense(128, activation='relu')(x1)\n",
        "x1 = tf.keras.layers.Dense(64, activation='relu')(x1)\n",
        "x1 = tf.keras.backend.sum(x1, axis=1, keepdims=False)\n",
        "\n",
        "multi_keyword_model = tf.keras.Model(inputs=inputs, outputs=x1)\n",
        "multi_keyword_model.summary()\n",
        "multi_keyword_model.output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XYFkxYBqqwU"
      },
      "source": [
        "# concat_MLP\n",
        "merged = tf.keras.layers.concatenate([Basic_MLP_model.output, multi_keyword_model.output])\n",
        "x2 = tf.keras.layers.BatchNormalization()(merged)\n",
        "x2 = tf.keras.layers.Dense(64, activation='relu')(x2)\n",
        "x2 = tf.keras.layers.Dense(64, activation='relu')(x2)\n",
        "x2 = tf.keras.layers.Dropout(0.5)(x2)\n",
        "x2 = tf.keras.layers.Dense(1, activation='relu')(x2)#予測値が正なのでreluを通す\n",
        "concat_MLP_model = tf.keras.Model([Basic_MLP_model.input, multi_keyword_model.input], x2)\n",
        "\n",
        "concat_MLP_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5bdFQ6JEy8S"
      },
      "source": [
        "## 関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQW-C7ojE2YE"
      },
      "source": [
        "import os\n",
        "import random\n",
        "def set_seed(seed):\n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = str(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjUVUOUzp18u"
      },
      "source": [
        "def history_mse_plot(history_with_1000epoch, stop_count, mse_of_train_mean, mse_of_train_10_mean, mse_of_Basic):\n",
        "  plt.figure(facecolor='w', figsize=(6, 4))\n",
        "  plt.plot(history_with_1000epoch.history[\"mse\"], )\n",
        "  plt.plot(history_with_1000epoch.history['val_mse'])\n",
        "  plt.axhline(mse_of_train_mean, xmin=0.05, xmax=0.95, color='green', lw=0.8)\n",
        "  plt.axhline(mse_of_train_10_mean, xmin=0.05, xmax=0.95, color='red', lw=0.8)\n",
        "  plt.axhline(mse_of_Basic, xmin=0.05, xmax=0.95, color='blue', lw=0.8)\n",
        "  plt.axvline(x=stop_count, ymin=0.05, ymax=0.95, color='orange', lw=0.8)\n",
        "  plt.title('Model mse')\n",
        "  plt.ylabel('mse')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.grid()\n",
        "  plt.legend(['Train', 'Validation', 'Tr mean', 'Tr_10 mean', 'Basic'], loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHQsMMiGqLsm"
      },
      "source": [
        "def predict_plot(df, pred_test, pred_train, pred_with_Basic):\n",
        "  plt.figure(facecolor='w', figsize=(24, 9))\n",
        "  plt.plot(df[\"No\"], df[\"favorite_count\"])#seriesもOK\n",
        "  plt.plot(df[\"No\"][len(pred_test):], pred_train, color='orange')\n",
        "  plt.plot(df[\"No\"][0:len(pred_test)], pred_test, color='red')\n",
        "  plt.grid()\n",
        "\n",
        "  plt.figure(facecolor='w', figsize=(16, 6))\n",
        "  plt.plot(df[\"No\"][0:len(pred_test)], df[\"favorite_count\"][0:len(pred_test)])#seriesもOK\n",
        "  plt.plot(df[\"No\"][0:len(pred_test)], pred_test, color='red')\n",
        "  plt.plot(df[\"No\"][0:len(pred_test)], pred_with_Basic, color='orange')\n",
        "  plt.grid()\n",
        "  plt.legend(['actual', 'Text', 'Basic'], loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nnwwNRnIS-P"
      },
      "source": [
        "def get_baseline(y_test, y_train):\n",
        "  y_10_mean = np.mean(y_train[0:10])\n",
        "  y_all_mean = np.mean(y_train)\n",
        "  y_10_means = np.array([y_10_mean for i in range(y_test.shape[0])])\n",
        "  y_all_means = np.array([y_all_mean for i in range(y_test.shape[0])])\n",
        "\n",
        "  y_10_mse = mean_squared_error(y_test, y_10_means)\n",
        "  y_all_mse = mean_squared_error(y_test, y_all_means)\n",
        "  return y_all_mean, y_10_mean, y_all_mse, y_10_mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvZk6ImWqVw7"
      },
      "source": [
        "## compile,fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fplQVVVXql7Q"
      },
      "source": [
        "def MLP(X_test, X_train, y_test, y_train, epochs=1000):\n",
        "  with tf.device('/device:GPU:0'):\n",
        "\n",
        "    # MLP_for_Basic--------------------\n",
        "    basic_size = 7\n",
        "\n",
        "    Basic_MLP_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(32, input_shape=(basic_size,)),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "        tf.keras.layers.Dense(32),\n",
        "        tf.keras.layers.Activation('relu')\n",
        "    ])\n",
        "\n",
        "    # MLP_for_multi_keyword----------\n",
        "    vec_size = 200\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=(None, vec_size))\n",
        "    x1 = tf.keras.layers.Masking(mask_value=0.0)(inputs)\n",
        "    x1 = tf.keras.layers.Dense(128, activation='relu')(x1)\n",
        "    x1 = tf.keras.layers.Dense(64, activation='relu')(x1)\n",
        "    x1 = tf.keras.backend.sum(x1, axis=1, keepdims=False)\n",
        "    multi_keyword_model = tf.keras.Model(inputs=inputs, outputs=x1)\n",
        "\n",
        "    # concat_MLP\n",
        "    merged = tf.keras.layers.concatenate([Basic_MLP_model.output, multi_keyword_model.output])\n",
        "    x2 = tf.keras.layers.BatchNormalization()(merged)\n",
        "    x2 = tf.keras.layers.Dense(64, activation='relu')(x2)\n",
        "    x2 = tf.keras.layers.Dense(64, activation='relu')(x2)\n",
        "    x2 = tf.keras.layers.Dropout(0.5)(x2)\n",
        "    x2 = tf.keras.layers.Dense(1, activation='relu')(x2)#予測値は正なのでreluを通す\n",
        "    concat_MLP_model = tf.keras.Model([Basic_MLP_model.input, multi_keyword_model.input], x2)\n",
        "\n",
        "    set_seed(42)\n",
        "    concat_MLP_model.compile(optimizer='adam', \n",
        "                  loss='mse', \n",
        "                  metrics=[tf.keras.metrics.RootMeanSquaredError(), \"mse\", \"mae\"])\n",
        "\n",
        "    history = concat_MLP_model.fit(X_train, y_train,\n",
        "                        batch_size=24,\n",
        "                        epochs=epochs, \n",
        "                        verbose=1, \n",
        "                        shuffle=True,\n",
        "                        validation_data=(X_test, y_test))\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-R1Wy1sGPbi"
      },
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
        "def MLP_early(X_test, X_train, y_test, y_train, epochs=1000):\n",
        "  with tf.device('/device:GPU:0'):\n",
        "\n",
        "    # MLP_for_Basic--------------------\n",
        "    basic_size = 7\n",
        "\n",
        "    Basic_MLP_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(32, input_shape=(basic_size,)),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "        tf.keras.layers.Dense(32),\n",
        "        tf.keras.layers.Activation('relu')\n",
        "    ])\n",
        "\n",
        "    # MLP_for_multi_keyword----------\n",
        "    vec_size = 200\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=(None, vec_size))\n",
        "    x1 = tf.keras.layers.Masking(mask_value=0.0)(inputs)\n",
        "    x1 = tf.keras.layers.Dense(128, activation='relu')(x1)\n",
        "    x1 = tf.keras.layers.Dense(64, activation='relu')(x1)\n",
        "    x1 = tf.keras.backend.sum(x1, axis=1, keepdims=False)\n",
        "    multi_keyword_model = tf.keras.Model(inputs=inputs, outputs=x1)\n",
        "\n",
        "    # concat_MLP\n",
        "    merged = tf.keras.layers.concatenate([Basic_MLP_model.output, multi_keyword_model.output])\n",
        "    x2 = tf.keras.layers.BatchNormalization()(merged)\n",
        "    x2 = tf.keras.layers.Dense(64, activation='relu')(x2)\n",
        "    x2 = tf.keras.layers.Dense(64, activation='relu')(x2)\n",
        "    x2 = tf.keras.layers.Dropout(0.5)(x2)\n",
        "    x2 = tf.keras.layers.Dense(1, activation='relu')(x2)\n",
        "    concat_MLP_model = tf.keras.Model([Basic_MLP_model.input, multi_keyword_model.input], x2)\n",
        "\n",
        "    set_seed(42)\n",
        "    concat_MLP_model.compile(optimizer='adam', \n",
        "                  loss='mse', \n",
        "                  metrics=[tf.keras.metrics.RootMeanSquaredError(), \"mse\", \"mae\"])\n",
        "    \n",
        "    history = concat_MLP_model.fit(X_train, y_train,\n",
        "                        batch_size=24,\n",
        "                        epochs=epochs, \n",
        "                        verbose=1, \n",
        "                        shuffle=True,\n",
        "                        callbacks=early_stop,\n",
        "                        validation_data=(X_test, y_test)\n",
        "\n",
        "  return history, concat_MLP_model.predict(X_train), concat_MLP_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UUTjMt6Bwpq"
      },
      "source": [
        "# user1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av6mmxQwBwpw"
      },
      "source": [
        "user_path = userIDs[0]\n",
        "user_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQLzhajWcT4k"
      },
      "source": [
        "mse_of_Basic = result_simple_and_multi_keyword[user_path][\"basic_mse\"]\n",
        "pred_with_Basic = result_simple_and_multi_keyword[user_path][\"basic_predict\"]\n",
        "\n",
        "df_Basic = dict_df_basic[user].drop(columns=[\"datetime\", \"year\", \"month\", \"day\", \"hour\", \"weekday\", \"unix_time\"], axis=1)\n",
        "df_Text = dict_series_multi_keyword[user]\n",
        "if df_Basic.shape[0] != df_Text.shape[0]:\n",
        "  error_list.append(user)\n",
        "y = df_Basic['favorite_count'].values\n",
        "X_basic = df_Basic.drop(columns=\"favorite_count\", axis=1)\n",
        "X_basic = s_scaler.fit_transform(X_basic)#arrayになる\n",
        "X_text = np.array(df_Text.values.tolist()) #df_Text.valuesだと，array(array())となる\n",
        "\n",
        "X_basic_test, X_basic_train, X_text_test, X_text_train, y_test, y_train = train_test_split(X_basic, X_text, y, shuffle=False, test_size=0.95)\n",
        "train_mean, train_10_mean, mse_of_train_mean, mse_of_train_10_mean = get_baseline(y_test, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6L9goPUBwpy"
      },
      "source": [
        "X_train = [X_basic_train, X_text_train]\n",
        "X_test = [X_basic_test, X_text_test]\n",
        "history_with_1000epoch = MLP(X_test, X_train, y_test, y_train, epochs=1000)\n",
        "history, pred_train, pred_test = MLP_early(X_test, X_train, y_test, y_train, epochs=1000)\n",
        "pred_train = np.ravel(pred_train)\n",
        "pred_test = np.ravel(pred_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk4B9xmvgs6m"
      },
      "source": [
        "print(\"訓練データ:\", X_basic_train.shape, X_text_train.shape, \"テストデータ:\", X_basic_test.shape, X_text_test.shape)\n",
        "history_mse_plot(history_with_1000epoch, len(history.history[\"loss\"]), mse_of_train_mean, mse_of_train_10_mean, mse_of_Basic)\n",
        "predict_plot(df_Basic, pred_test, pred_train, pred_with_Basic)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}